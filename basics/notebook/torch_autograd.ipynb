{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53547c91-bad0-4018-b8f0-2b534368dfd5",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n",
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, with input x, parameters w and b, and some loss function. It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd79df4b-039b-4dc1-b94e-11d610cd60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470d579-881e-4bc1-b4f5-7fb1131a10ac",
   "metadata": {},
   "source": [
    "You can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5bcf049-d8e7-4651-972a-e054f563a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f1e2c6acfa0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f1e2c6ac1c0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b05a48-47a3-48a5-a7c8-7d8e6b3b4203",
   "metadata": {},
   "source": [
    "# Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa79ad2-d5de-4552-b6c7-6381b34a8e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1100, 0.2907, 0.0603],\n",
      "        [0.1100, 0.2907, 0.0603],\n",
      "        [0.1100, 0.2907, 0.0603],\n",
      "        [0.1100, 0.2907, 0.0603],\n",
      "        [0.1100, 0.2907, 0.0603]])\n",
      "tensor([0.1100, 0.2907, 0.0603])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1676957-7474-47f0-86a7-496ea49dc261",
   "metadata": {},
   "source": [
    "We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1c8bd-f937-472c-903e-d04b27c09f6e",
   "metadata": {},
   "source": [
    "# Disabling Gradient Tracking\n",
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d08a110-880b-4480-8018-bed2ade28134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e064c68-0c52-4360-900c-e8be50b4e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Another way to achieve the same result is to use the detach() method on the tensor:\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab06ab3-721e-4286-aae1-604c1cee251e",
   "metadata": {},
   "source": [
    "## Note\n",
    "DAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n",
    "\n",
    "Previously we were calling backward() function without parameters. This is essentially equivalent to calling backward(torch.tensor(1.0)), which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22665724-75c8-43ef-9723-4dd91bd93080",
   "metadata": {},
   "source": [
    "# Optional Reading: Tensor Gradients and Jacobian Products\n",
    "In many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called Jacobian product, and not the actual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8488120e-f2f2-4fed-95bd-8629bbdac975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call \n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "Second call \n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp + 1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call \\n\", inp.grad)\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"Second call \\n\", inp.grad)\n",
    "\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66c31b-6df1-4ee1-ac22-dbf4498f9f6b",
   "metadata": {},
   "source": [
    "## Note\n",
    "Notice that when we call backward for the second time with the same argument, the value of the gradient is different. This happens because when doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the grad property before. In real-life training an optimizer helps us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1335cd-e8f6-4c51-97fa-c16f74189ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/hangwu/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5fc46d-5e52-44b5-b404-a7a12650cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00167c5-8935-4a46-a6a1-e391f209ecfd",
   "metadata": {},
   "source": [
    "We use the model’s prediction and the corresponding label to calculate the error (loss). The next step is to backpropagate this error through the network. Backward propagation is kicked off when we call .backward() on the error tensor. Autograd then calculates and stores the gradients for each model parameter in the parameter’s .grad attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12bb197e-3812-4a63-8a88-becefbc41f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()  # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff8898-0211-4751-92dc-518f99e7c80f",
   "metadata": {},
   "source": [
    "Next, we load an optimizer, in this case SGD with a learning rate of 0.01 and momentum of 0.9.\n",
    "\n",
    "We register all parameters of the model in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b355166-8416-4fd8-b345-13b92833be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9d11b-2ee6-4a6b-925d-5cc3a23db749",
   "metadata": {},
   "source": [
    "Finally, we call .step() to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in .grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c877910-c192-4ef2-a9ab-db2597fb8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602f423-7036-4ace-9c17-51e67819084f",
   "metadata": {},
   "source": [
    "# Differentiation in Autograd\n",
    "Let’s take a look at how autograd collects gradients. We create two tensors a and b with requires_grad=True. This signals to autograd that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e28fc5-de7e-4e91-9037-597beb6f8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75dcb706-8c10-43ca-b3b9-be4eb7a817d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d444f0e-dc89-474e-9fd5-0a286fb652ae",
   "metadata": {},
   "source": [
    "Let’s assume a and b to be parameters of an NN, and Q to be the error. In NN training, we want gradients of the error w.r.t. parameters, i.e.\n",
    "\n",
    "When we call .backward() on Q, autograd calculates these gradients and stores them in the respective tensors’ .grad attribute.\n",
    "\n",
    "We need to explicitly pass a gradient argument in Q.backward() because it is a vector. gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t. itself, i.e.\n",
    "\n",
    "\\frac{dQ}{dQ} = 1\n",
    "\n",
    "Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like Q.sum().backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa87dfd7-a91a-4f97-bebb-66a04c7b8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee70e96-ac31-4ab8-8913-85b118a7034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6696cfed-6b9f-4745-aa3c-67e2429b9b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d64fc5-eb75-4c4a-a202-3b29ef72f57e",
   "metadata": {},
   "source": [
    "In a NN, parameters that don’t compute gradients are usually called frozen parameters. It is useful to “freeze” part of your model if you know in advance that you won’t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "Another common usecase where exclusion from the DAG is important is for finetuning a pretrained network\n",
    "\n",
    "In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. Let’s walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40aaf4fd-3055-4604-bb22-68ae6ed993f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b265389-24e1-4034-a2b6-b5c4d22bd4cd",
   "metadata": {},
   "source": [
    "Let’s say we want to finetune the model on a new dataset with 10 labels. In resnet, the classifier is the last linear layer model.fc. We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e1b6d7-9ac4-4449-8041-1dcfbe7507d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971f000-0f20-4fb5-873e-2e16bce13e31",
   "metadata": {},
   "source": [
    "Now all parameters in the model, except the parameters of model.fc, are frozen. The only parameters that compute gradients are the weights and bias of model.fc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13401b1c-e006-45ec-b951-64ca11c2692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43045a-ab60-4e1e-a224-45e4355e79d9",
   "metadata": {},
   "source": [
    "Notice although we register all the parameters in the optimizer, the only parameters that are computing gradients (and hence updated in gradient descent) are the weights and bias of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e685e-c79e-478e-b61c-538b88e8a156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc59dd4-f8ab-42b2-9a74-b59ae828b9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3e608-288b-4c4f-ab84-a762593cd389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613de9a-4893-4c56-bbc6-867c7d6fd11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
